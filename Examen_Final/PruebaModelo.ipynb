{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCqkcfczupF1",
        "outputId": "03917b1f-66fa-4b97-c5c9-bd5c8380b141"
      },
      "outputs": [],
      "source": [
        "# # Cargar Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-Oa24sxuvJZ",
        "outputId": "402f901b-ca83-49f0-814d-3ac9d4f317a4"
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchtext transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WSpK7MzSysYA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained_model_name='distilbert-base-uncased'):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained(pretrained_model_name)\n",
        "        self.bert = DistilBertModel.from_pretrained(pretrained_model_name)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        output = self.bert(input_ids, attention_mask=attention_mask)[0][:, 0, :]  # extract the [CLS] token representation\n",
        "        logits = self.fc(output)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dl_wKHTfyx2c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset  # Añadir esta línea para importar la clase Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data.iloc[idx]['texto']\n",
        "        label = self.data.iloc[idx]['carrera']\n",
        "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt', return_attention_mask=True)\n",
        "        return {'input_ids': encoding['input_ids'].squeeze(), 'attention_mask': encoding['attention_mask'].squeeze(), 'label': torch.tensor(label)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f9JfYdV3y0BH"
      },
      "outputs": [],
      "source": [
        "# Cargar tu conjunto de datos desde el archivo CSV\n",
        "dataset_path = './datos_carreras_3.csv'\n",
        "df = pd.read_csv(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4gC3T5Zwy9On"
      },
      "outputs": [],
      "source": [
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tj6KrAQpy-KP"
      },
      "outputs": [],
      "source": [
        "# Inicializar el modelo y el tokenizer\n",
        "num_classes = 4  # Número de clases en tu problema\n",
        "model = TextClassifier(num_classes)\n",
        "tokenizer = model.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "928ec5wYzACL"
      },
      "outputs": [],
      "source": [
        "# Crear instancias de CustomDataset\n",
        "train_dataset = CustomDataset(train_df, tokenizer)\n",
        "test_dataset = CustomDataset(test_df, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RrKIrqK1zDKK"
      },
      "outputs": [],
      "source": [
        "# Crear DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZNsUTLs-u0b",
        "outputId": "a904b36d-4d35-4ab9-a401-a0c03a9c0adf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101,  2474,  2801,  ...,     0,     0,     0],\n",
            "        [  101,  2474,  7367,  ...,     0,     0,     0],\n",
            "        [  101, 15697, 13340,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  4078,  2906,  ...,     0,     0,     0],\n",
            "        [  101, 21864, 10624,  ...,     0,     0,     0],\n",
            "        [  101,  9530, 18886,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'label': tensor([3, 3, 3, 3, 1, 3, 3, 2, 2, 1, 3, 2, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 3, 2,\n",
            "        1, 1, 3, 0, 2, 2, 2, 0, 1, 2, 0, 0, 3, 2, 3, 0, 1, 2, 2, 0, 1, 1, 1, 1,\n",
            "        0, 2, 1, 2, 0, 0, 2, 0, 2, 2, 3, 0, 0, 2, 3, 3])}\n"
          ]
        }
      ],
      "source": [
        "for batch in train_dataloader:\n",
        "    print(batch)\n",
        "    break  # solo mostramos el primer lote para evitar demasiada salida\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7AtlIm1a0JYQ"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataloader, test_dataloader, num_epochs=12, learning_rate=1e-4, save_every=3):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch in train_dataloader:\n",
        "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader:\n",
        "                input_ids, labels = batch['input_ids'].to(device), batch['label'].to(device)\n",
        "                logits = model(input_ids)\n",
        "                _, predicted = torch.max(logits, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Save model every 'save_every' epochs\n",
        "        if (epoch + 1) % save_every == 0:\n",
        "            save_path = f'./drive/MyDrive/SIS421/ClasificacionTexto/model_checkpoint_epoch_{epoch + 1}.pth'\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss.item(),\n",
        "                'accuracy': accuracy\n",
        "            }, save_path)\n",
        "            print(f'Model saved at epoch {epoch + 1}.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBrgDJnfoKV5",
        "outputId": "e5174a2a-64a7-4909-8063-9eabf38a9467"
      },
      "outputs": [],
      "source": [
        "# # Entrenar el modelo con guardado cada 3 epochs\n",
        "# train_model(model, train_dataloader, test_dataloader, num_epochs=3, learning_rate=1e-4, save_every=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCJIgUEUsZSj",
        "outputId": "2991637b-fb14-4f57-b4a1-a97988cb373a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from epoch 12, Loss: 0.0970, Accuracy: 0.8120\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def load_model(model, optimizer, checkpoint_path):\n",
        "    # Usa map_location='cpu' al cargar el modelo\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    accuracy = checkpoint['accuracy']\n",
        "    print(f'Model loaded from epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "\n",
        "# Especifica la ruta del archivo del modelo que deseas cargar\n",
        "model_path_to_load = './model_checkpoint_epoch_12.pth'  # Cambia la ruta según sea necesario\n",
        "\n",
        "# Crea una nueva instancia del modelo (asegúrate de que tenga la misma arquitectura que el modelo original)\n",
        "loaded_model = TextClassifier(num_classes)\n",
        "\n",
        "# Crea una nueva instancia del optimizador (asegúrate de que tenga los mismos parámetros que el modelo original)\n",
        "loaded_optimizer = torch.optim.AdamW(loaded_model.parameters(), lr=1e-4)  # Ajusta el learning rate según sea necesario\n",
        "\n",
        "# Carga el modelo y el optimizador desde el archivo\n",
        "load_model(loaded_model, loaded_optimizer, model_path_to_load)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OTAbfZ2H-dl7"
      },
      "outputs": [],
      "source": [
        "# Dar etiquetas a los resultados para mejorar la presentacion\n",
        "def label_mapping(label):\n",
        "    mapping = {\n",
        "        0: 'Ing. En Sistemas',\n",
        "        1: 'Ing. En Diseño y Animacion Digital',\n",
        "        2: 'Ing. En Ciencias de la Computación',\n",
        "        3: 'Ing. En Tecnologías de la Información y Seguridad'\n",
        "    }\n",
        "    return mapping[label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppaVqCuRsh1q",
        "outputId": "17ffd022-7cd1-4f88-c666-2a9662bd0bb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto: siempre he estado interesado en la seguridad de la informacion y como se resguardan los datos\n",
            "Etiqueta Predicha con modelo cargado: Ing. En Tecnologías de la Información y Seguridad\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Función para realizar predicciones con el modelo cargado\n",
        "def predict_with_loaded_model(model, tokenizer, text):\n",
        "    # Tokenizar el texto de entrada\n",
        "    inputs = tokenizer(text, truncation=True, padding='max_length', max_length=model.tokenizer.model_max_length, return_tensors='pt', return_attention_mask=True)\n",
        "\n",
        "    # Obtener predicciones del modelo cargado\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "\n",
        "    # Obtener la etiqueta predicha\n",
        "    _, predicted_class = torch.max(logits, 1)\n",
        "\n",
        "    return label_mapping(predicted_class.item())\n",
        "\n",
        "# Ejemplo de texto para probar el modelo cargado\n",
        "texto_a_probar_cargado = \"siempre he estado interesado en la seguridad de la informacion y como se resguardan los datos\"\n",
        "\n",
        "# Probar el modelo con el texto de ejemplo\n",
        "predicted_label_cargado = predict_with_loaded_model(loaded_model, tokenizer, texto_a_probar_cargado)\n",
        "\n",
        "# Imprimir la etiqueta predicha de forma legible\n",
        "print(f\"Texto: {texto_a_probar_cargado}\")\n",
        "print(f\"Etiqueta Predicha con modelo cargado: {predicted_label_cargado}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfV0hd9Ztrhq",
        "outputId": "983f40c3-0241-42e3-a3f8-6b2313407d68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto: siempre he estado interesado en ingenieria social\n",
            "Etiqueta Predicha con modelo cargado: Ing. En Tecnologías de la Información y Seguridad\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de texto para probar el modelo cargado\n",
        "texto_a_probar_cargado = \"siempre he estado interesado en ingenieria social\"\n",
        "\n",
        "# Probar el modelo con el texto de ejemplo\n",
        "predicted_label_cargado = predict_with_loaded_model(loaded_model, tokenizer, texto_a_probar_cargado)\n",
        "\n",
        "# Imprimir la etiqueta predicha de forma legible\n",
        "print(f\"Texto: {texto_a_probar_cargado}\")\n",
        "print(f\"Etiqueta Predicha con modelo cargado: {predicted_label_cargado}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKRA5u7TtwsH",
        "outputId": "884d1c66-5fed-4cb8-8126-5927f5a71e4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto: Adoro ver cómo funcionan los sistemas y cómo interactúan entre sí\n",
            "Etiqueta Predicha con modelo cargado: Ing. En Sistemas\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de texto para probar el modelo cargado\n",
        "texto_a_probar_cargado = \"Adoro ver cómo funcionan los sistemas y cómo interactúan entre sí\"\n",
        "\n",
        "# Probar el modelo con el texto de ejemplo\n",
        "predicted_label_cargado = predict_with_loaded_model(loaded_model, tokenizer, texto_a_probar_cargado)\n",
        "\n",
        "# Imprimir la etiqueta predicha de forma legible\n",
        "print(f\"Texto: {texto_a_probar_cargado}\")\n",
        "print(f\"Etiqueta Predicha con modelo cargado: {predicted_label_cargado}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mHYXeVXt79V",
        "outputId": "578f021c-aaf8-4d25-91bf-5bfd7e22c7c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto: Desde pequeño siempre me llamo la atencion la programacion y participe en eventos de programacion competitiva\n",
            "Etiqueta Predicha con modelo cargado: Ing. En Ciencias de la Computación\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de texto para probar el modelo cargado\n",
        "texto_a_probar_cargado = \"Desde pequeño siempre me llamo la atencion la programacion y participe en eventos de programacion competitiva\"\n",
        "\n",
        "# Probar el modelo con el texto de ejemplo\n",
        "predicted_label_cargado = predict_with_loaded_model(loaded_model, tokenizer, texto_a_probar_cargado)\n",
        "\n",
        "# Imprimir la etiqueta predicha de forma legible\n",
        "print(f\"Texto: {texto_a_probar_cargado}\")\n",
        "print(f\"Etiqueta Predicha con modelo cargado: {predicted_label_cargado}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bEgT_gpuF0F",
        "outputId": "bdf75100-57ce-4957-c996-c74cca56b460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto: La producción de contenido visual siempre ha sido una pasion mia\n",
            "Etiqueta Predicha con modelo cargado: Ing. En Diseño y Animacion Digital\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de texto para probar el modelo cargado\n",
        "texto_a_probar_cargado = \"La producción de contenido visual siempre ha sido una pasion mia\"\n",
        "\n",
        "# Probar el modelo con el texto de ejemplo\n",
        "predicted_label_cargado = predict_with_loaded_model(loaded_model, tokenizer, texto_a_probar_cargado)\n",
        "\n",
        "# Imprimir la etiqueta predicha de forma legible\n",
        "print(f\"Texto: {texto_a_probar_cargado}\")\n",
        "print(f\"Etiqueta Predicha con modelo cargado: {predicted_label_cargado}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwGql8D8uONP",
        "outputId": "19866f9a-56e8-4cd9-80b7-9bafab45c7ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto: Tengo interes en aprender sobre redes, ingenieria social y como se mueve la informacion\n",
            "Etiqueta Predicha con modelo cargado: Ing. En Tecnologías de la Información y Seguridad\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de texto para probar el modelo cargado\n",
        "texto_a_probar_cargado = \"Tengo interes en aprender sobre redes, ingenieria social y como se mueve la informacion\"\n",
        "\n",
        "# Probar el modelo con el texto de ejemplo\n",
        "predicted_label_cargado = predict_with_loaded_model(loaded_model, tokenizer, texto_a_probar_cargado)\n",
        "\n",
        "# Imprimir la etiqueta predicha de forma legible\n",
        "print(f\"Texto: {texto_a_probar_cargado}\")\n",
        "print(f\"Etiqueta Predicha con modelo cargado: {predicted_label_cargado}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto: adoro programar, los puzzles y jugar videojuegos\n",
            "Etiqueta Predicha con modelo cargado: Ing. En Ciencias de la Computación\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de texto para probar el modelo cargado\n",
        "texto_a_probar_cargado = \"adoro programar, los puzzles y jugar videojuegos\"\n",
        "\n",
        "# Probar el modelo con el texto de ejemplo\n",
        "predicted_label_cargado = predict_with_loaded_model(loaded_model, tokenizer, texto_a_probar_cargado)\n",
        "\n",
        "# Imprimir la etiqueta predicha de forma legible\n",
        "print(f\"Texto: {texto_a_probar_cargado}\")\n",
        "print(f\"Etiqueta Predicha con modelo cargado: {predicted_label_cargado}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
